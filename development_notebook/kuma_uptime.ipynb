{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline for Pterodactyl Minecraft Servers Analysis\n",
    "\n",
    "## Index\n",
    "\n",
    "- Install requierements\n",
    "- Import libraries and setup key variables\n",
    "- Get the Uptime Kuma information\n",
    "- Load data into the Postgres database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requierements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install uptime-kuma-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and setup key variables\n",
    "Remember to add you own credentials in the .env file for them to be loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from uptime_kuma_api import UptimeKumaApi, MonitorType\n",
    "\n",
    "# Load .env file credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "dbname = os.getenv('POSTGRES_DBNAME')\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "connection = f'postgresql://{user}:{password}@{host}:{port}/{dbname}'\n",
    "\n",
    "# Kuma connection\n",
    "kuma_token = os.getenv('TOKEN')\n",
    "kuma_url = os.getenv('KUMA_URL')\n",
    "kuma_user = os.getenv('KUMA_USER')\n",
    "kuma_pass = os.getenv('KUMA_PASS')\n",
    "\n",
    "# Connect to Kuma\n",
    "api = UptimeKumaApi('https://kuma.stiv.tech')\n",
    "#api.login(kuma_user, kuma_pass)\n",
    "api.login_by_token(kuma_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Uptime Kuma information\n",
    "Average ping, uptime from 24 hours and 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ping result\n",
    "ping_result = api.avg_ping()\n",
    "\n",
    "# Get the uptime result\n",
    "uptime_result = api.uptime()\n",
    "\n",
    "# Mapping the server names by extracting the id and server names from monitors in kuma and putting them into a dictionary\n",
    "# (this is because the avg ping and uptime only give us the id and the info, not the server name)\n",
    "monitors = api.get_monitors()\n",
    "monitor_dict = {monitor.get(\"id\"): monitor.get(\"name\") for monitor in monitors}\n",
    "\n",
    "# Replace server numbers with names in the result\n",
    "ping_result_with_names = {monitor_dict.get(key, f\"Unknown Server {key}\"): value for key, value in ping_result.items()}\n",
    "uptime_result_with_names = {monitor_dict.get(key, f\"Unknown Server {key}\"): value for key, value in uptime_result.items()}\n",
    "# Convert ping and uptime to DataFrames\n",
    "ping_df = pd.DataFrame(list(ping_result_with_names.items()), columns=['server_name', 'avg_ping'])\n",
    "uptime_df = pd.DataFrame(list(uptime_result_with_names.items()), columns=['server_name', 'Uptime'])\n",
    "\n",
    "# Transform the NaN ping values to 0\n",
    "ping_df = ping_df.fillna('0')\n",
    "\n",
    "# Split the 'Uptime' column into '24 Hours' and '30 Days' columns\n",
    "uptime_df[['uptime_24hours', 'uptime_30days']] = pd.DataFrame(uptime_df['Uptime'].tolist(), index=uptime_df.index)\n",
    "# Drop the original 'Uptime' column and the '30 days column'\n",
    "uptime_df = uptime_df.drop(columns=['Uptime', 'uptime_30days'])\n",
    "\n",
    "# Multiply the percentage columns by 100\n",
    "#uptime_df['uptime_24hours'] *= 100\n",
    "#uptime_df['uptime_30days'] *= 100\n",
    "\n",
    "# Merge the DataFrames and add the 'creation_time' column\n",
    "final_df = pd.merge(ping_df, uptime_df, on='server_name', how='inner')\n",
    "final_df['creation_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Filter out rows with 'Unknown Server'\n",
    "final_df_filtered = final_df[~final_df['server_name'].str.startswith('Unknown Server')]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "final_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have the final_df_filtered DataFrame\n",
    "\n",
    "# Convert 'server_name' to string type (necessary for further comparisons)\n",
    "final_df_filtered['server_name'] = final_df_filtered['server_name'].astype(str)\n",
    "\n",
    "# Separate the DataFrame into nodes and servers\n",
    "nodes_df = final_df_filtered[\n",
    "    final_df_filtered['server_name'].apply(\n",
    "        lambda x: str(x).split(' | ')[0].isdigit() and int(str(x).split(' | ')[0]) <= 100\n",
    "    )\n",
    "]\n",
    "nodes_df['server_name'] = nodes_df['server_name'].apply(lambda x: str(x).split(' | ')[0])\n",
    "nodes_df.rename(columns={'server_name': 'node_id'}, inplace=True)\n",
    "\n",
    "servers_df = final_df_filtered[\n",
    "    ~final_df_filtered['server_name'].apply(\n",
    "        lambda x: str(x).split(' | ')[0].isdigit() and int(str(x).split(' | ')[0]) <= 100\n",
    "    )\n",
    "]\n",
    "servers_df['server_name'] = servers_df['server_name'].apply(lambda x: str(x).split(' | ')[0])\n",
    "servers_df.rename(columns={'server_name': 'server_identifier'}, inplace=True)\n",
    "\n",
    "# Filter out the row with 'Nodes' in servers_df\n",
    "servers_df = servers_df[servers_df['server_identifier'] != 'Nodes']\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"Nodes:\")\n",
    "print(nodes_df)\n",
    "\n",
    "print(\"\\nServers:\")\n",
    "print(servers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into the Postgres database\n",
    "Into kuma table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database and upload all new logs into table\n",
    "SCHEMA = 'kuma'\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "\n",
    "# Start a new transaction\n",
    "    trans = conn.begin()\n",
    "\n",
    "    try:\n",
    "        # Load all new activity into postgres\n",
    "        nodes_df.to_sql(name = 'nodes', schema = SCHEMA, con = conn, if_exists='append', index=False)\n",
    "        servers_df.to_sql(name = 'servers', schema = SCHEMA, con = conn, if_exists='append', index=False)\n",
    "\n",
    "        # Commit the transaction\n",
    "        trans.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Rollback the transaction on exception\n",
    "        print('!!! [ERROR IN DATABASE QUERIES] !!!')\n",
    "        trans.rollback()\n",
    "        print('Transaction has been rolled back')\n",
    "        print(f'Error occurred during transaction:\\n{e}')\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
